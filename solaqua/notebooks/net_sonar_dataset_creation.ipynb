{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## net_sonar dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image selection\n",
    "The extraction and exporting (after they have been enhanced) of images happens in the solaqua_sonar_dataprocessing.ipynb. They are exported to their own folder ../data/exports/sonar/net_sonar/labelrady/images\n",
    "\n",
    "\n",
    "We select the following bags: (200 imgs per)\n",
    "- bag1:  2024-08-20_13-57-42 - Net is medium away. Oritentation varies. Some \"folds\" effect at the end. Some fish here and there. \n",
    "- bag2:  2024-08-20_17-02-00 - Net is close, far at the end. Stable data of the net. Few fishes on the inside. \n",
    "- bag3:  2024-08-20_13-55-34 - Net is far away, therefore weaker signal of net. Orientation varies a little. Few fish at the end. \n",
    "- bag4:  2024-08-20_14-31-29 - Net is far away. Few fish before the end. Net is a little wobbly. \n",
    "- bag5:  2024-08-20_14-38-37 - Net is medium away. Mostly stable data, very clear lines. Very few fishes on inside.\n",
    "- bag6:  2024-08-20_14-34-07 - Net is medium-close away. Orientation constant. More unstable at the end. Very few fishes on inside.\n",
    "- bag7:  2024-08-20_14-16-05 - Net is medium-close away. Unstable net - orientation and wobbles. Some fishes on inside.\n",
    "- bag8:  2024-08-20_18-50-22 - Net is close. Stable. No fish inside.\n",
    "- bag9:  2024-08-20_16-47-54 - Net is medium and cloase away. Unstable at times, inconsistent. Some fish inside.\n",
    "- bag10: 2024-08-20_17-14-36 - Net is medium-close. Fish inside, ALOT at the end, difficult to understand net at the end. \n",
    "\n",
    "\n",
    "Naming convention:\n",
    "- \\<bagX>_<ts_ns>.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprocessing and Enhancement\n",
    "\n",
    "This is done in solaqua_sonar_dataprocessing.ipynb. We use the config-file found in configs/net_sonar_labelingview_config.py. We do this processing to easier to the labeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is manually done using Label Studio. We now have 2000 labels in the data/net_labes_raw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train, val and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Done ===\n",
      "train: 1400\n",
      "val:   200\n",
      "test:  400\n",
      "skipped: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "# ---- Config ----\n",
    "REPO_ROOT     = Path(\"..\")\n",
    "DATA_ROOT     = REPO_ROOT / \"data\"\n",
    "SRC_LABEL_ROOT = DATA_ROOT / \"net_labels_raw\"        # <-- raw labels live here\n",
    "DST_DATA_ROOT  = DATA_ROOT / \"net_sonar\"             # <-- new dataset root\n",
    "DST_IMG_ROOT   = DST_DATA_ROOT / \"images\"            # created empty for now\n",
    "DST_LBL_ROOT   = DST_DATA_ROOT / \"labels\"            # labels/{train,val,test}\n",
    "\n",
    "# Split by BAG NUMBER (from filenames like: <id>-bag3_<timestamp>.txt)\n",
    "TRAIN_BAGS = {1, 2, 4, 6, 7, 8, 9}\n",
    "VAL_BAGS   = {5}\n",
    "TEST_BAGS  = {3, 10}\n",
    "\n",
    "LABEL_EXTS   = {\".txt\"}    # YOLO label files\n",
    "PATTERN      = re.compile(r\"-bag(\\d+)_\")  # captures the bag number\n",
    "\n",
    "# ---- Helpers ----\n",
    "def ensure_dirs():\n",
    "    (DST_LBL_ROOT / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "    (DST_LBL_ROOT / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "    (DST_LBL_ROOT / \"test\").mkdir(parents=True, exist_ok=True)\n",
    "    DST_IMG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def split_for_bag(bag_num: int) -> Optional[str]:\n",
    "    if bag_num in TRAIN_BAGS:\n",
    "        return \"train\"\n",
    "    if bag_num in VAL_BAGS:\n",
    "        return \"val\"\n",
    "    if bag_num in TEST_BAGS:\n",
    "        return \"test\"\n",
    "    return None\n",
    "\n",
    "def new_filename(original_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove the unimportant-id prefix up to and including the first '-'.\n",
    "    Example: '0a05d92c-bag3_1724...txt' -> 'bag3_1724...txt'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return original_name.split(\"-\", 1)[1]\n",
    "    except IndexError:\n",
    "        return original_name\n",
    "\n",
    "def copy_or_link(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "def build_split():\n",
    "    ensure_dirs()\n",
    "    counts = Counter()\n",
    "    skipped = 0\n",
    "    unparsable = []\n",
    "\n",
    "    files = [p for p in SRC_LABEL_ROOT.rglob(\"*\") if p.is_file() and p.suffix.lower() in LABEL_EXTS]\n",
    "\n",
    "    for p in files:\n",
    "        m = PATTERN.search(p.name)\n",
    "        if not m:\n",
    "            unparsable.append(p.name)\n",
    "            continue\n",
    "\n",
    "        bag_num = int(m.group(1))\n",
    "        split = split_for_bag(bag_num)\n",
    "        if split is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Remove prefix\n",
    "        out_name = new_filename(p.name)\n",
    "        dst = DST_LBL_ROOT / split / out_name\n",
    "\n",
    "        # Handle collision by appending __dup#\n",
    "        if dst.exists():\n",
    "            stem, ext = dst.stem, dst.suffix\n",
    "            k = 1\n",
    "            while True:\n",
    "                candidate = dst.with_name(f\"{stem}__dup{k}{ext}\")\n",
    "                if not candidate.exists():\n",
    "                    dst = candidate\n",
    "                    break\n",
    "                k += 1\n",
    "\n",
    "        copy_or_link(p, dst)\n",
    "        counts[split] += 1\n",
    "\n",
    "    print(\"=== Done ===\")\n",
    "    print(f\"train: {counts['train']}\")\n",
    "    print(f\"val:   {counts['val']}\")\n",
    "    print(f\"test:  {counts['test']}\")\n",
    "    print(f\"skipped: {skipped}\")\n",
    "\n",
    "    if unparsable:\n",
    "        print(\"\\n[WARN] Could not parse bag number from these filenames:\")\n",
    "        for n in unparsable[:20]:\n",
    "            print(\"  -\", n)\n",
    "        if len(unparsable) > 20:\n",
    "            print(f\"  ... and {len(unparsable)-20} more\")\n",
    "\n",
    "# Execute\n",
    "build_split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root added to sys.path: /cluster/home/henrban/SOLAQUA-UOD/solaqua\n",
      "Indexed label timestamps per split/bag:\n",
      "  train: 1400 labels across bags [1, 2, 4, 6, 7, 8, 9]\n",
      "  val: 200 labels across bags [5]\n",
      "  test: 400 labels across bags [3, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 203\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mImages written under: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mIMAGES_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# go!\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m, in \u001b[0;36mrun_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split, bag_map \u001b[38;5;129;01min\u001b[39;00m split_index\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bag_num, ts_set \u001b[38;5;129;01min\u001b[39;00m bag_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 191\u001b[0m         stats \u001b[38;5;241m=\u001b[39m \u001b[43mexport_images_for_bag\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m         grand\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_saved\u001b[39m\u001b[38;5;124m\"\u001b[39m: stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m    193\u001b[0m         grand\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_expected\u001b[39m\u001b[38;5;124m\"\u001b[39m: stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "Cell \u001b[0;32mIn[1], line 135\u001b[0m, in \u001b[0;36mexport_images_for_bag\u001b[0;34m(split, bag_num, timestamps)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;129;01mand\u001b[39;00m frame_idx \u001b[38;5;241m<\u001b[39m MAX_FRAMES_PER_BAG \u001b[38;5;129;01mand\u001b[39;00m errors \u001b[38;5;241m<\u001b[39m MAX_CONSEC_ERRORS:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m         M_raw, t_ns \u001b[38;5;241m=\u001b[39m \u001b[43mload_sonoptix_frame_from_bag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         errors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/SOLAQUA-UOD/solaqua/utils/loader.py:20\u001b[0m, in \u001b[0;36mload_sonoptix_frame_from_bag\u001b[0;34m(bag_path, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mReturns float32 array shaped (H,W) for one SonoptixECHO frame.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bag_path\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBag not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbag_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m AnyReader([bag_path]) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[1;32m     21\u001b[0m     conns \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mconnections \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mtopic \u001b[38;5;241m==\u001b[39m TOPIC \u001b[38;5;129;01mor\u001b[39;00m c\u001b[38;5;241m.\u001b[39mmsgtype \u001b[38;5;241m==\u001b[39m MSGTYPE]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conns:\n",
      "File \u001b[0;32m~/SOLAQUA-UOD/.venv/lib64/python3.9/site-packages/rosbags/highlevel/anyreader.py:178\u001b[0m, in \u001b[0;36mAnyReader.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Open rosbags when entering contextmanager.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/SOLAQUA-UOD/.venv/lib64/python3.9/site-packages/rosbags/highlevel/anyreader.py:126\u001b[0m, in \u001b[0;36mAnyReader.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m reader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaders:\n\u001b[0;32m--> 126\u001b[0m         \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m         rollback\u001b[38;5;241m.\u001b[39mappend(reader)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReaderErrors \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SOLAQUA-UOD/.venv/lib64/python3.9/site-packages/rosbags/rosbag1/reader.py:432\u001b[0m, in \u001b[0;36mReader.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks[chunk_info\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunk()\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunk_info\u001b[38;5;241m.\u001b[39mconnection_counts)):\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_index_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexes \u001b[38;5;241m=\u001b[39m {x\u001b[38;5;241m.\u001b[39mid: \u001b[38;5;28msorted\u001b[39m(indexes[x\u001b[38;5;241m.\u001b[39mid]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections}\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    437\u001b[0m     Connection(\u001b[38;5;241m*\u001b[39mx[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m], \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexes[x\u001b[38;5;241m.\u001b[39mid]), \u001b[38;5;241m*\u001b[39mx[\u001b[38;5;241m6\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections\n\u001b[1;32m    438\u001b[0m ]\n",
      "File \u001b[0;32m~/SOLAQUA-UOD/.venv/lib64/python3.9/site-packages/rosbags/rosbag1/reader.py:578\u001b[0m, in \u001b[0;36mReader.read_index_data\u001b[0;34m(self, pos, indexes)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read index data from position.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mThe implementation purposely avoids the generic Header class and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbio\n\u001b[0;32m--> 578\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m55\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_data_header_offsets:\n\u001b[1;32m    580\u001b[0m     (size,) \u001b[38;5;241m=\u001b[39m deserialize_uint32(buf, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build images for the net_sonar dataset by matching label timestamps\n",
    "# Run in notebooks/net_sonar_creation.ipynb  (Python 3.9+)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repo root to sys.path so \"utils and config\" can be imported\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print(\"Repo root added to sys.path:\", REPO_ROOT)\n",
    "\n",
    "# from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Optional, Dict, Set\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# your utils\n",
    "from utils.loader import load_sonoptix_frame_from_bag\n",
    "from utils.sonar_visualization import save_cone_view_image, enhance_cfc_style\n",
    "from configs.cfc_gray_config import cfc_gray_config\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS / CONFIG\n",
    "# ----------------------------\n",
    "REPO_ROOT   = Path(\"..\")\n",
    "DATA_ROOT   = REPO_ROOT / \"data\"\n",
    "\n",
    "BAGS_ROOT   = DATA_ROOT / \"bags\"                   # where the .bag (or similar) files are\n",
    "LABELS_ROOT = DATA_ROOT / \"net_sonar\" / \"labels\"   # labels/{train,val,test}\n",
    "IMAGES_ROOT = DATA_ROOT / \"net_sonar\" / \"images\"   # images/{train,val,test}\n",
    "\n",
    "# ensure split folders exist\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    (IMAGES_ROOT / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# bag number -> timestamped stem (adjust if your stems differ)\n",
    "BAG_ID_TO_STEM: Dict[int, str] = {\n",
    "    1: \"2024-08-20_13-57-42\",\n",
    "    2: \"2024-08-20_17-02-00\",\n",
    "    3: \"2024-08-20_13-55-34\",\n",
    "    4: \"2024-08-20_14-31-29\",\n",
    "    5: \"2024-08-20_14-38-37\",\n",
    "    6: \"2024-08-20_14-34-07\",\n",
    "    7: \"2024-08-20_14-16-05\",\n",
    "    8: \"2024-08-20_18-50-22\",\n",
    "    9: \"2024-08-20_16-47-54\",\n",
    "    10:\"2024-08-20_17-14-36\",\n",
    "}\n",
    "\n",
    "# enhancer + sonar config (you already have this in your notebook/environment)\n",
    "ENHANCER = enhance_cfc_style\n",
    "CFG =  cfc_gray_config \n",
    "\n",
    "# loader control (stop conditions)\n",
    "MAX_FRAMES_PER_BAG = 10000     # hard upper bound; will stop earlier when all labels found\n",
    "MAX_CONSEC_ERRORS  = 500        # break if we see too many consecutive load failures (end-of-bag)\n",
    "PRINT_EVERY        = 200\n",
    "\n",
    "# label filename pattern: bagX_timestamp.txt  (X = 1..10, timestamp = ns)\n",
    "LBL_NAME_RE = re.compile(r\"^bag(\\d+)_(\\d+)\\.txt$\")\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "def index_label_timestamps(labels_root: Path):\n",
    "    \"\"\"\n",
    "    Read labels/{train,val,test} and build:\n",
    "      split_to_bag_ts[split][bag_num] = set({timestamps})\n",
    "    \"\"\"\n",
    "    split_to_bag_ts: Dict[str, Dict[int, Set[int]]] = {\n",
    "        \"train\": defaultdict(set),\n",
    "        \"val\":   defaultdict(set),\n",
    "        \"test\":  defaultdict(set),\n",
    "    }\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        split_dir = labels_root / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        for p in split_dir.glob(\"*.txt\"):\n",
    "            m = LBL_NAME_RE.match(p.name)\n",
    "            if not m:\n",
    "                # ignore unparsable names\n",
    "                continue\n",
    "            bag_num = int(m.group(1))\n",
    "            ts = int(m.group(2))\n",
    "            split_to_bag_ts[split][bag_num].add(ts)\n",
    "    return split_to_bag_ts\n",
    "\n",
    "\n",
    "def find_bag_file(stem: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Try to locate the bag file for a given timestamped stem.\n",
    "    We search for files starting with the stem (e.g., '2024-08-20_13-57-42')\n",
    "    and prefer something that contains 'video' if present.\n",
    "    \"\"\"\n",
    "    candidates = list(BAGS_ROOT.glob(f\"{stem}*\"))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # prefer a file whose name contains 'video'\n",
    "    for c in candidates:\n",
    "        if \"video\" in c.stem.lower():\n",
    "            return c\n",
    "    # otherwise take the first\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def export_images_for_bag(split: str, bag_num: int, timestamps: Set[int]) -> Counter:\n",
    "    \"\"\"\n",
    "    Iterate frames of a bag and export cone-view jpgs whose t_ns matches label timestamps.\n",
    "    Saves to data/net_sonar/images/{split}/bag{bag_num}_{t_ns}.jpg\n",
    "    \"\"\"\n",
    "    stem = BAG_ID_TO_STEM.get(bag_num)\n",
    "    if stem is None:\n",
    "        tqdm.write(f\"[WARN] No stem for bag{bag_num}\")\n",
    "        return Counter()\n",
    "\n",
    "    bag_path = find_bag_file(stem)\n",
    "    if bag_path is None:\n",
    "        tqdm.write(f\"[WARN] Bag file not found for stem '{stem}' in {BAGS_ROOT}\")\n",
    "        return Counter()\n",
    "\n",
    "    out_dir = IMAGES_ROOT / split\n",
    "    remaining = set(timestamps)  # copy so we can remove when saved\n",
    "    saved = 0\n",
    "    errors = 0\n",
    "    frame_idx = 0\n",
    "    stats = Counter()\n",
    "\n",
    "    pbar = tqdm(total=len(remaining), desc=f\"{split}: bag{bag_num} -> images\", leave=False)\n",
    "    try:\n",
    "        while remaining and frame_idx < MAX_FRAMES_PER_BAG and errors < MAX_CONSEC_ERRORS:\n",
    "            try:\n",
    "                M_raw, t_ns = load_sonoptix_frame_from_bag(bag_path, frame_idx)\n",
    "            except Exception:\n",
    "                errors += 1\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "\n",
    "            # reset error streak if we successfully loaded a frame\n",
    "            errors = 0\n",
    "\n",
    "            if M_raw is None or not isinstance(M_raw, np.ndarray):\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "\n",
    "            if t_ns in remaining:\n",
    "                out_name = f\"bag{bag_num}_{t_ns}.jpg\"\n",
    "                out_path = out_dir / out_name\n",
    "\n",
    "                save_cone_view_image(\n",
    "                    M_raw,\n",
    "                    CFG,\n",
    "                    out_path=out_path,\n",
    "                    use_enhanced=True,\n",
    "                    enhancer=ENHANCER,\n",
    "                )\n",
    "\n",
    "                remaining.remove(t_ns)\n",
    "                saved += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "            frame_idx += 1\n",
    "\n",
    "            if saved % PRINT_EVERY == 0 and saved > 0:\n",
    "                tqdm.write(f\"{split} bag{bag_num}: saved {saved}/{len(timestamps)}\")\n",
    "\n",
    "    finally:\n",
    "        pbar.close()\n",
    "\n",
    "    if remaining:\n",
    "        tqdm.write(f\"[INFO] {split} bag{bag_num}: {len(remaining)} labeled timestamps not found in bag frames.\")\n",
    "\n",
    "    stats[\"saved\"] = saved\n",
    "    stats[\"expected\"] = len(timestamps)\n",
    "    stats[\"missing\"] = len(remaining)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    split_index = index_label_timestamps(LABELS_ROOT)\n",
    "    print(\"Indexed label timestamps per split/bag:\")\n",
    "    for split, d in split_index.items():\n",
    "        total = sum(len(v) for v in d.values())\n",
    "        print(f\"  {split}: {total} labels across bags {sorted(d.keys())}\")\n",
    "\n",
    "    grand = Counter()\n",
    "    for split, bag_map in split_index.items():\n",
    "        for bag_num, ts_set in bag_map.items():\n",
    "            stats = export_images_for_bag(split, bag_num, ts_set)\n",
    "            grand.update({f\"{split}_saved\": stats[\"saved\"]})\n",
    "            grand.update({f\"{split}_expected\": stats[\"expected\"]})\n",
    "            if stats[\"missing\"]:\n",
    "                grand.update({f\"{split}_missing\": stats[\"missing\"]})\n",
    "\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    for k, v in sorted(grand.items()):\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(f\"\\nImages written under: {IMAGES_ROOT}\")\n",
    "\n",
    "# go!\n",
    "run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# === Your final split (timestamps) ===\n",
    "train_bags = [\n",
    "    \"2024-08-20_13-57-42\",  # bag1\n",
    "    \"2024-08-20_17-02-00\",  # bag2\n",
    "    \"2024-08-20_14-31-29\",  # bag4\n",
    "    \"2024-08-20_14-34-07\",  # bag6\n",
    "    \"2024-08-20_14-16-05\",  # bag7\n",
    "    \"2024-08-20_18-50-22\",  # bag8\n",
    "    \"2024-08-20_16-47-54\",  # bag9\n",
    "]\n",
    "\n",
    "val_bags = [\n",
    "    \"2024-08-20_14-38-37\",  # bag5\n",
    "]\n",
    "\n",
    "test_bags = [\n",
    "    \"2024-08-20_13-55-34\",  # bag3\n",
    "    \"2024-08-20_17-14-36\",  # bag10\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
